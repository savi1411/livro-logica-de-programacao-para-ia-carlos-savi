{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shZa9MQ62wz7"
      },
      "outputs": [],
      "source": [
        "# Instalar a biblioteca NumPy\n",
        "!pip install numpy\n",
        "\n",
        "# Instalar a biblioteca Pandas\n",
        "!pip install pandas\n",
        "\n",
        "# Instalar a biblioteca Matplotlib\n",
        "!pip install matplotlib\n",
        "\n",
        "# Instalar a biblioteca Scikit-learn\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Instalar a biblioteca TensorFlow (que inclui Keras)\n",
        "!pip install tensorflow\n",
        "\n",
        "# Instalar a biblioteca NLTK\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Livro: Lógica de Programação para IA\n",
        "Autor: Carlos Alberto Savi\n",
        "Capítulo: Bibliotecas Essenciais para IA com Python\n",
        "'''\n",
        "\n",
        "## Biblioteca NumPy\n",
        "\n",
        "# Operações Básicas com Arrays\n",
        "import numpy as np\n",
        "\n",
        "# Criando um array\n",
        "arr = np.array([1, 2, 3, 4, 5])\n",
        "\n",
        "# Operações básicas\n",
        "soma = np.sum(arr)\n",
        "media = np.mean(arr)\n",
        "produto = np.prod(arr)\n",
        "\n",
        "print(\"Array:\", arr)\n",
        "print(\"Soma:\", soma)\n",
        "print(\"Média:\", media)\n",
        "print(\"Produto:\", produto)\n",
        "\n",
        "## Biblioteca Pandas\n",
        "\n",
        "# Manipulação de dados com DataFrames\n",
        "import pandas as pd\n",
        "\n",
        "# Criando um DataFrame\n",
        "dados = {\n",
        "    'Produto': ['Produto A', 'Produto B', 'Produto C'],\n",
        "    'Vendas': [100, 150, 200],\n",
        "    'Lucro': [30, 50, 70]\n",
        "}\n",
        "df = pd.DataFrame(dados)\n",
        "\n",
        "# Operações básicas\n",
        "total_vendas = df['Vendas'].sum()\n",
        "media_lucro = df['Lucro'].mean()\n",
        "\n",
        "print(\"DataFrame:\\n\", df)\n",
        "print(\"Total de Vendas:\", total_vendas)\n",
        "print(\"Média de Lucro:\", media_lucro)\n",
        "\n",
        "## Biblioteca Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dados de vendas\n",
        "meses = ['Jan', 'Fev', 'Mar', 'Abr', 'Mai']\n",
        "vendas = [100, 120, 130, 115, 140]\n",
        "\n",
        "# Criando o gráfico\n",
        "plt.plot(meses, vendas)\n",
        "plt.xlabel('Meses')\n",
        "plt.ylabel('Vendas')\n",
        "plt.title('Vendas ao longo dos meses')\n",
        "plt.show()\n",
        "\n",
        "## Biblioteca Scikit-learn\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Carregando o dataset Iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Dividindo os dados em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Treinando o modelo KNN\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Fazendo previsões\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Calculando a acurácia\n",
        "acuracia = accuracy_score(y_test, y_pred)\n",
        "print(\"Acurácia do modelo KNN:\", acuracia)\n",
        "\n",
        "## Biblioteca TensorFlow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Carregando o dataset MNIST\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
        "\n",
        "# Criando o modelo\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilando o modelo\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Treinando o modelo\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32)\n",
        "\n",
        "# Avaliando o modelo\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Acurácia do modelo:\", accuracy)\n",
        "\n",
        "## Biblioteca Keras\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "# Ajustando os dados para o formato necessário pela CNN\n",
        "X_train = X_train.reshape(-1, 28, 28, 1)\n",
        "X_test = X_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Criando o modelo\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilando o modelo\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Treinando o modelo\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32)\n",
        "\n",
        "# Avaliando o modelo\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Acurácia do modelo:\", accuracy)\n",
        "\n",
        "## Biblioteca NLTK (Natural Language Toolkit)\n",
        "\n",
        "# Instalação e Importação\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Carrega o léxico de sentimentos\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Inicializando o analisador de sentimentos\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Textos de exemplo\n",
        "textos = [\n",
        "    \"I love this product, it is amazing!\",\n",
        "    \"The service was terrible and I do not recommend it.\",\n",
        "    \"It is an average item, nothing special.\"\n",
        "]\n",
        "\n",
        "# Análise de sentimentos\n",
        "for texto in textos:\n",
        "    print(f\"Texto: {texto}\")\n",
        "    ss = sid.polarity_scores(texto)\n",
        "    for k in ss:\n",
        "        print(f\"{k}: {ss[k]}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "ZovVaXsH3km5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}